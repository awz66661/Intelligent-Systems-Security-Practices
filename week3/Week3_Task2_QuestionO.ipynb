{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164f81cc",
   "metadata": {},
   "source": [
    "# Task2: 基于PyTorch框架的手写数字识别\n",
    "## 引入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004bf64b-0af9-4c00-99b2-1b4405e8f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d527ca",
   "metadata": {},
   "source": [
    "## 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc22903",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epoches = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb7104",
   "metadata": {},
   "source": [
    "## Re-implement MLP\n",
    "利用PyTorch的内置神经网络模块（torch.nn.Module的子类），在MLP类中实现两个函数：\n",
    "+ 在__init__函数中，定义一个网络结构为[784-245-128-10]的MLP模型结构\n",
    "+ 在forward函数中，实现该MLP模型的前向传播过程\n",
    "\n",
    "下面是一些供你参考/可能用到的API函数：\n",
    "\n",
    "- torch.nn.Linear(*in_features*, *out_features*, *bias=True*, *device=None*, *dtype=None*) [\n",
    "  Link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "  - in_features: 输入网络层的特征维度\n",
    "  - out_features: 输出网络层的特征维度\n",
    "- torch.nn.Module.forward(**input*) [Link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward)\n",
    "  - 执行模型的前向过程，继承nn.Module类的类实例可以直接通过变量名加括号实现forward函数的调用，不需要写明调用forward函数\n",
    "  - 如定义了MLP(nn.Module)，则对于mlp = MLP()，可以通过mlp(**input*)调用\n",
    "- torch.Tensor.reshape(*shape*) [Link](https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html)\n",
    "  - shape: 当前tensor希望修改为的形状，如(2, 2)或(-1, 3)\n",
    "    - -1指该维度大小根据原数据维度大小和其它给定维度大小计算得到，至多可以给一个-1\n",
    "- torch.nn.Sigmoid() [Link](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1193adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: 定义上文要求的MLP模型结构\n",
    "        self.fc1 = nn.Linear(784, 245)\n",
    "        self.fc2 = nn.Linear(245, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: 定义MLP模型的前向过程\n",
    "        xL = x.view(-1, 784)\n",
    "        h1 = self.fc1(xL)\n",
    "        h2 = self.fc2(h1)\n",
    "        o = self.fc3(h2)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0bf5a",
   "metadata": {},
   "source": [
    "## 示例化MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d07082",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ebfa8",
   "metadata": {},
   "source": [
    "## 定义损失函数、优化算法\n",
    "\n",
    "- torch.nn.CrossEntropyLoss(*weight=None*, *size_average=None*, *ignore_index=- 100*, *reduce=None*, *reduction='mean'*, *label_smoothing=0.0*) [Link](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "  - loss.backward(): loss通过特定的计算方式获得，如调用CrossEntropyLoss；对loss执行backward()会为计算图中涉及的tensor反向计算梯度，累积到tensor.grad上\n",
    "- torch.optim.SGD(*params*, *lr=<required parameter>*, *momentum=0*, *dampening=0*, *weight_decay=0*, *nesterov=False*, ***, *maximize=False*, *foreach=None*, *differentiable=False*)  [Link](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "  - params: 需优化的参数Tensor\n",
    "  - lr: 参数优化的学习率\n",
    "  - zero_grad(): 清空相关参数上累积的梯度\n",
    "  - step(): 根据tensor上累积的梯度，进行一次参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae0d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f3d31",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "\n",
    "- 自动下载MNIST数据集到./MNIST路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17757476-51c0-4d83-bd6b-a052e6c4d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./MNIST\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root=\"./MNIST\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cae6c5",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "此处关于loss和optimizer的用法请参考上一段落的API介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c74b85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:07<00:00, 63.68it/s, epoch=0, train_loss=0.457]\n",
      "100%|██████████| 468/468 [00:08<00:00, 54.79it/s, epoch=1, train_loss=0.274]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.64it/s, epoch=2, train_loss=0.363]\n",
      "100%|██████████| 468/468 [00:09<00:00, 48.94it/s, epoch=3, train_loss=0.42] \n",
      "100%|██████████| 468/468 [00:09<00:00, 49.19it/s, epoch=4, train_loss=0.628]\n",
      "100%|██████████| 468/468 [00:09<00:00, 48.39it/s, epoch=5, train_loss=0.636]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.24it/s, epoch=6, train_loss=0.39] \n",
      "100%|██████████| 468/468 [00:09<00:00, 51.33it/s, epoch=7, train_loss=0.372]\n",
      "100%|██████████| 468/468 [00:09<00:00, 48.61it/s, epoch=8, train_loss=0.504]\n",
      "100%|██████████| 468/468 [00:10<00:00, 46.66it/s, epoch=9, train_loss=0.253]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.16it/s, epoch=10, train_loss=0.42] \n",
      "100%|██████████| 468/468 [00:09<00:00, 49.01it/s, epoch=11, train_loss=0.312]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.96it/s, epoch=12, train_loss=0.634]\n",
      "100%|██████████| 468/468 [00:09<00:00, 46.82it/s, epoch=13, train_loss=0.282] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.22it/s, epoch=14, train_loss=0.23] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.26it/s, epoch=15, train_loss=0.441]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.27it/s, epoch=16, train_loss=0.328]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.99it/s, epoch=17, train_loss=0.229]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.90it/s, epoch=18, train_loss=0.607] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.50it/s, epoch=19, train_loss=0.446]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.52it/s, epoch=20, train_loss=0.26] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.20it/s, epoch=21, train_loss=0.132] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.40it/s, epoch=22, train_loss=0.169]\n",
      "100%|██████████| 468/468 [00:11<00:00, 42.08it/s, epoch=23, train_loss=0.388]\n",
      "100%|██████████| 468/468 [00:11<00:00, 42.24it/s, epoch=24, train_loss=0.196] \n",
      "100%|██████████| 468/468 [00:11<00:00, 39.62it/s, epoch=25, train_loss=0.575]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.38it/s, epoch=26, train_loss=0.245]\n",
      "100%|██████████| 468/468 [00:11<00:00, 41.42it/s, epoch=27, train_loss=0.44] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.84it/s, epoch=28, train_loss=0.468]\n",
      "100%|██████████| 468/468 [00:11<00:00, 41.75it/s, epoch=29, train_loss=0.417] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.39it/s, epoch=30, train_loss=0.776]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.39it/s, epoch=31, train_loss=0.193]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.86it/s, epoch=32, train_loss=0.255]\n",
      "100%|██████████| 468/468 [00:12<00:00, 38.65it/s, epoch=33, train_loss=0.409]\n",
      "100%|██████████| 468/468 [00:11<00:00, 41.16it/s, epoch=34, train_loss=0.676]\n",
      "100%|██████████| 468/468 [00:16<00:00, 27.94it/s, epoch=35, train_loss=0.473]\n",
      "100%|██████████| 468/468 [00:11<00:00, 42.19it/s, epoch=36, train_loss=0.316] \n",
      "100%|██████████| 468/468 [00:12<00:00, 36.51it/s, epoch=37, train_loss=0.461]\n",
      "100%|██████████| 468/468 [00:11<00:00, 39.85it/s, epoch=38, train_loss=0.366]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.63it/s, epoch=39, train_loss=0.263]\n",
      "100%|██████████| 468/468 [00:11<00:00, 41.71it/s, epoch=40, train_loss=0.366] \n",
      "100%|██████████| 468/468 [00:12<00:00, 36.59it/s, epoch=41, train_loss=0.194] \n",
      "100%|██████████| 468/468 [00:12<00:00, 37.62it/s, epoch=42, train_loss=0.462] \n",
      "100%|██████████| 468/468 [00:11<00:00, 39.85it/s, epoch=43, train_loss=0.403]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.57it/s, epoch=44, train_loss=0.531] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.21it/s, epoch=45, train_loss=0.668] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.12it/s, epoch=46, train_loss=0.555]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.96it/s, epoch=47, train_loss=0.146]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.69it/s, epoch=48, train_loss=0.324]\n",
      "100%|██████████| 468/468 [00:12<00:00, 37.76it/s, epoch=49, train_loss=0.467]\n",
      "100%|██████████| 468/468 [00:12<00:00, 36.28it/s, epoch=50, train_loss=0.403] \n",
      "100%|██████████| 468/468 [00:12<00:00, 37.58it/s, epoch=51, train_loss=0.393]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.16it/s, epoch=52, train_loss=0.577]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.28it/s, epoch=53, train_loss=0.244]\n",
      "100%|██████████| 468/468 [00:11<00:00, 42.15it/s, epoch=54, train_loss=0.285]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.72it/s, epoch=55, train_loss=0.624] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.64it/s, epoch=56, train_loss=0.695] \n",
      "100%|██████████| 468/468 [00:10<00:00, 42.79it/s, epoch=57, train_loss=0.44] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.68it/s, epoch=58, train_loss=0.299] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.50it/s, epoch=59, train_loss=0.267] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.67it/s, epoch=60, train_loss=0.638]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.48it/s, epoch=61, train_loss=0.257] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.86it/s, epoch=62, train_loss=0.332] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.85it/s, epoch=63, train_loss=0.37] \n",
      "100%|██████████| 468/468 [00:10<00:00, 45.18it/s, epoch=64, train_loss=0.788]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.18it/s, epoch=65, train_loss=0.196] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.71it/s, epoch=66, train_loss=0.244]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.96it/s, epoch=67, train_loss=0.317] \n",
      "100%|██████████| 468/468 [00:10<00:00, 45.47it/s, epoch=68, train_loss=0.566] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.95it/s, epoch=69, train_loss=0.597]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.63it/s, epoch=70, train_loss=0.274] \n",
      "100%|██████████| 468/468 [00:10<00:00, 45.29it/s, epoch=71, train_loss=0.239] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.69it/s, epoch=72, train_loss=0.366] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.73it/s, epoch=73, train_loss=0.205]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.25it/s, epoch=74, train_loss=0.283] \n",
      "100%|██████████| 468/468 [00:10<00:00, 45.27it/s, epoch=75, train_loss=0.662]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.83it/s, epoch=76, train_loss=0.455]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.88it/s, epoch=77, train_loss=0.327]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.97it/s, epoch=78, train_loss=0.492] \n",
      "100%|██████████| 468/468 [00:11<00:00, 42.34it/s, epoch=79, train_loss=0.146]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.35it/s, epoch=80, train_loss=0.176]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.31it/s, epoch=81, train_loss=0.272]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.51it/s, epoch=82, train_loss=0.243]\n",
      "100%|██████████| 468/468 [00:11<00:00, 39.97it/s, epoch=83, train_loss=0.682] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.51it/s, epoch=84, train_loss=0.354] \n",
      "100%|██████████| 468/468 [00:15<00:00, 29.53it/s, epoch=85, train_loss=0.444]\n",
      "100%|██████████| 468/468 [00:12<00:00, 36.06it/s, epoch=86, train_loss=0.343] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.49it/s, epoch=87, train_loss=0.34] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.71it/s, epoch=88, train_loss=2.02] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.75it/s, epoch=89, train_loss=0.312] \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.53it/s, epoch=90, train_loss=0.199] \n",
      "100%|██████████| 468/468 [00:13<00:00, 34.17it/s, epoch=91, train_loss=0.159] \n",
      "100%|██████████| 468/468 [00:12<00:00, 38.02it/s, epoch=92, train_loss=0.257]\n",
      "100%|██████████| 468/468 [00:12<00:00, 37.29it/s, epoch=93, train_loss=0.326]\n",
      "100%|██████████| 468/468 [00:12<00:00, 36.93it/s, epoch=94, train_loss=0.553] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.23it/s, epoch=95, train_loss=0.285]\n",
      "100%|██████████| 468/468 [00:11<00:00, 41.26it/s, epoch=96, train_loss=0.547]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.03it/s, epoch=97, train_loss=0.236]\n",
      "100%|██████████| 468/468 [00:08<00:00, 55.47it/s, epoch=98, train_loss=0.397]\n",
      "100%|██████████| 468/468 [00:07<00:00, 64.20it/s, epoch=99, train_loss=0.514] \n"
     ]
    }
   ],
   "source": [
    "mlp.train()\n",
    "\n",
    "for e in range(epoches):\n",
    "    t = tqdm(train_loader)\n",
    "    for img, label in t:\n",
    "        # Forward img and compute loss\n",
    "        pred = mlp(img)\n",
    "        loss = criterion(pred, label)\n",
    "        \n",
    "        # TODO: 基于优化器的使用方法，完成反向梯度传播、参数更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_postfix(epoch=e, train_loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060fa17",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "\n",
    "- torch.argmax(*input*, *dim*, *keepdim=False*) [Link](https://pytorch.org/docs/stable/generated/torch.argmax.html)\n",
    "  - input: 计算基于的tensor\n",
    "  - dim: 希望按哪个维度求max下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97bb449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 76.87it/s, test_acc=0.895]\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "\n",
    "correct_cnt, sample_cnt = 0, 0\n",
    "\n",
    "t = tqdm(test_loader)\n",
    "for img, label in t:\n",
    "    # Predict label for img\n",
    "    img = img.reshape(img.shape[0], -1)\n",
    "    pred = mlp(img)\n",
    "    pred_label = pred.argmax(dim=1)\n",
    "    \n",
    "    correct_cnt += (pred_label == label).sum().item()\n",
    "    sample_cnt += pred_label.shape[0]\n",
    "\n",
    "    t.set_postfix(test_acc=correct_cnt/sample_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251bbfb",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "\n",
    "- 将完成训练的模型保存到服务器的model/目录下\n",
    "\n",
    "- ModelScope服务器端无法长久保存文件，因此请及时下载、本地保存你完成的代码，以及模型的参数文件（model/mlp.pt）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113ac32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model/'):\n",
    "    os.mkdir('model/')\n",
    "\n",
    "torch.save(mlp.state_dict(), 'model/mlpO.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0af82aca95c4c45fbffe75f913fb6ef834a764cc29274537834de14d8772671d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
