{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164f81cc",
   "metadata": {},
   "source": [
    "# Task2: 基于PyTorch框架的手写数字识别\n",
    "## 引入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004bf64b-0af9-4c00-99b2-1b4405e8f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d527ca",
   "metadata": {},
   "source": [
    "## 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc22903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epoches = 100\n",
    "batch_size = 128\n",
    "print(torch.cuda.is_available())\n",
    "# test torch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb7104",
   "metadata": {},
   "source": [
    "## Re-implement MLP\n",
    "利用PyTorch的内置神经网络模块（torch.nn.Module的子类），在MLP类中实现两个函数：\n",
    "+ 在__init__函数中，定义一个网络结构为[784-245-128-10]的MLP模型结构\n",
    "+ 在forward函数中，实现该MLP模型的前向传播过程\n",
    "\n",
    "下面是一些供你参考/可能用到的API函数：\n",
    "\n",
    "- torch.nn.Linear(*in_features*, *out_features*, *bias=True*, *device=None*, *dtype=None*) [\n",
    "  Link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "  - in_features: 输入网络层的特征维度\n",
    "  - out_features: 输出网络层的特征维度\n",
    "- torch.nn.Module.forward(**input*) [Link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward)\n",
    "  - 执行模型的前向过程，继承nn.Module类的类实例可以直接通过变量名加括号实现forward函数的调用，不需要写明调用forward函数\n",
    "  - 如定义了MLP(nn.Module)，则对于mlp = MLP()，可以通过mlp(**input*)调用\n",
    "- torch.Tensor.reshape(*shape*) [Link](https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html)\n",
    "  - shape: 当前tensor希望修改为的形状，如(2, 2)或(-1, 3)\n",
    "    - -1指该维度大小根据原数据维度大小和其它给定维度大小计算得到，至多可以给一个-1\n",
    "- torch.nn.Sigmoid() [Link](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1193adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: 定义上文要求的MLP模型结构\n",
    "        self.fc1 = nn.Linear(784, 245)\n",
    "        self.fc2 = nn.Linear(245, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: 定义MLP模型的前向过程\n",
    "        xL = x.view(-1, 784)\n",
    "        h1 = self.fc1(xL)\n",
    "        h2 = self.fc2(h1)\n",
    "        o = self.fc3(h2)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0bf5a",
   "metadata": {},
   "source": [
    "## 示例化MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d07082",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ebfa8",
   "metadata": {},
   "source": [
    "## 定义损失函数、优化算法\n",
    "\n",
    "- torch.nn.CrossEntropyLoss(*weight=None*, *size_average=None*, *ignore_index=- 100*, *reduce=None*, *reduction='mean'*, *label_smoothing=0.0*) [Link](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "  - loss.backward(): loss通过特定的计算方式获得，如调用CrossEntropyLoss；对loss执行backward()会为计算图中涉及的tensor反向计算梯度，累积到tensor.grad上\n",
    "- torch.optim.SGD(*params*, *lr=<required parameter>*, *momentum=0*, *dampening=0*, *weight_decay=0*, *nesterov=False*, ***, *maximize=False*, *foreach=None*, *differentiable=False*)  [Link](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "  - params: 需优化的参数Tensor\n",
    "  - lr: 参数优化的学习率\n",
    "  - zero_grad(): 清空相关参数上累积的梯度\n",
    "  - step(): 根据tensor上累积的梯度，进行一次参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae0d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f3d31",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "\n",
    "- 自动下载MNIST数据集到./MNIST路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17757476-51c0-4d83-bd6b-a052e6c4d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./MNIST\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root=\"./MNIST\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cae6c5",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "此处关于loss和optimizer的用法请参考上一段落的API介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c74b85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:09<00:00, 47.58it/s, epoch=0, train_loss=1.02] \n",
      "100%|██████████| 468/468 [00:09<00:00, 51.52it/s, epoch=1, train_loss=0.541]\n",
      "100%|██████████| 468/468 [00:09<00:00, 50.17it/s, epoch=2, train_loss=0.504]\n",
      "100%|██████████| 468/468 [00:09<00:00, 50.95it/s, epoch=3, train_loss=0.418]\n",
      "100%|██████████| 468/468 [00:09<00:00, 48.58it/s, epoch=4, train_loss=0.332]\n",
      "100%|██████████| 468/468 [00:09<00:00, 51.12it/s, epoch=5, train_loss=0.306]\n",
      "100%|██████████| 468/468 [00:09<00:00, 51.28it/s, epoch=6, train_loss=0.28] \n",
      "100%|██████████| 468/468 [00:09<00:00, 48.88it/s, epoch=7, train_loss=0.24] \n",
      "100%|██████████| 468/468 [00:09<00:00, 48.15it/s, epoch=8, train_loss=0.341]\n",
      "100%|██████████| 468/468 [00:09<00:00, 50.17it/s, epoch=9, train_loss=0.384]\n",
      "100%|██████████| 468/468 [00:09<00:00, 48.35it/s, epoch=10, train_loss=0.438]\n",
      "100%|██████████| 468/468 [00:10<00:00, 46.77it/s, epoch=11, train_loss=0.264]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.29it/s, epoch=12, train_loss=0.372]\n",
      "100%|██████████| 468/468 [00:10<00:00, 46.61it/s, epoch=13, train_loss=0.26] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.30it/s, epoch=14, train_loss=0.245]\n",
      "100%|██████████| 468/468 [00:10<00:00, 46.49it/s, epoch=15, train_loss=0.337]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.43it/s, epoch=16, train_loss=0.345]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.89it/s, epoch=17, train_loss=0.245]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.97it/s, epoch=18, train_loss=0.328]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.26it/s, epoch=19, train_loss=0.402]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.45it/s, epoch=20, train_loss=0.262]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.19it/s, epoch=21, train_loss=0.203]\n",
      "100%|██████████| 468/468 [00:10<00:00, 42.84it/s, epoch=22, train_loss=0.302]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.20it/s, epoch=23, train_loss=0.393]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.25it/s, epoch=24, train_loss=0.226]\n",
      "100%|██████████| 468/468 [00:10<00:00, 45.07it/s, epoch=25, train_loss=0.28] \n",
      "100%|██████████| 468/468 [00:11<00:00, 42.24it/s, epoch=26, train_loss=0.224] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.27it/s, epoch=27, train_loss=0.212]\n",
      "100%|██████████| 468/468 [00:10<00:00, 42.99it/s, epoch=28, train_loss=0.205]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.56it/s, epoch=29, train_loss=0.267] \n",
      "100%|██████████| 468/468 [00:11<00:00, 42.42it/s, epoch=30, train_loss=0.294]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.75it/s, epoch=31, train_loss=0.317] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.24it/s, epoch=32, train_loss=0.286] \n",
      "100%|██████████| 468/468 [00:10<00:00, 42.64it/s, epoch=33, train_loss=0.344] \n",
      "100%|██████████| 468/468 [00:13<00:00, 34.24it/s, epoch=34, train_loss=0.17] \n",
      "100%|██████████| 468/468 [00:13<00:00, 35.05it/s, epoch=35, train_loss=0.221]\n",
      "100%|██████████| 468/468 [00:11<00:00, 40.03it/s, epoch=36, train_loss=0.265]\n",
      "100%|██████████| 468/468 [00:12<00:00, 38.33it/s, epoch=37, train_loss=0.263] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.29it/s, epoch=38, train_loss=0.267]\n",
      "100%|██████████| 468/468 [00:10<00:00, 42.73it/s, epoch=39, train_loss=0.232] \n",
      "100%|██████████| 468/468 [00:11<00:00, 39.20it/s, epoch=40, train_loss=0.315] \n",
      "100%|██████████| 468/468 [00:11<00:00, 39.99it/s, epoch=41, train_loss=0.379]\n",
      "100%|██████████| 468/468 [00:12<00:00, 38.00it/s, epoch=42, train_loss=0.279] \n",
      "100%|██████████| 468/468 [00:10<00:00, 42.81it/s, epoch=43, train_loss=0.338]\n",
      "100%|██████████| 468/468 [00:10<00:00, 42.71it/s, epoch=44, train_loss=0.211] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.11it/s, epoch=45, train_loss=0.174] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.02it/s, epoch=46, train_loss=0.213] \n",
      "100%|██████████| 468/468 [00:11<00:00, 42.35it/s, epoch=47, train_loss=0.294] \n",
      "100%|██████████| 468/468 [00:11<00:00, 42.24it/s, epoch=48, train_loss=0.229]\n",
      "100%|██████████| 468/468 [00:12<00:00, 38.79it/s, epoch=49, train_loss=0.231]\n",
      "100%|██████████| 468/468 [00:12<00:00, 36.51it/s, epoch=50, train_loss=0.162]\n",
      "100%|██████████| 468/468 [00:11<00:00, 39.34it/s, epoch=51, train_loss=0.164] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.41it/s, epoch=52, train_loss=0.288]\n",
      "100%|██████████| 468/468 [00:11<00:00, 42.10it/s, epoch=53, train_loss=0.362]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.52it/s, epoch=54, train_loss=0.435] \n",
      "100%|██████████| 468/468 [00:10<00:00, 45.88it/s, epoch=55, train_loss=0.237] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.04it/s, epoch=56, train_loss=0.377] \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.42it/s, epoch=57, train_loss=0.18]  \n",
      "100%|██████████| 468/468 [00:09<00:00, 46.85it/s, epoch=58, train_loss=0.24]  \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.24it/s, epoch=59, train_loss=0.195] \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.82it/s, epoch=60, train_loss=0.23]  \n",
      "100%|██████████| 468/468 [00:09<00:00, 46.81it/s, epoch=61, train_loss=0.382] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.36it/s, epoch=62, train_loss=0.328] \n",
      "100%|██████████| 468/468 [00:09<00:00, 46.88it/s, epoch=63, train_loss=0.305] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.78it/s, epoch=64, train_loss=0.287] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.49it/s, epoch=65, train_loss=0.246] \n",
      "100%|██████████| 468/468 [00:09<00:00, 46.92it/s, epoch=66, train_loss=0.206]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.39it/s, epoch=67, train_loss=0.567] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.73it/s, epoch=68, train_loss=0.274]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.44it/s, epoch=69, train_loss=0.369]\n",
      "100%|██████████| 468/468 [00:09<00:00, 46.98it/s, epoch=70, train_loss=0.272] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.60it/s, epoch=71, train_loss=0.135] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.71it/s, epoch=72, train_loss=0.285] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.72it/s, epoch=73, train_loss=0.344] \n",
      "100%|██████████| 468/468 [00:10<00:00, 45.81it/s, epoch=74, train_loss=0.232]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.09it/s, epoch=75, train_loss=0.264] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.00it/s, epoch=76, train_loss=0.245] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.15it/s, epoch=77, train_loss=0.384] \n",
      "100%|██████████| 468/468 [00:09<00:00, 46.83it/s, epoch=78, train_loss=0.167] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.19it/s, epoch=79, train_loss=0.26]  \n",
      "100%|██████████| 468/468 [00:10<00:00, 43.80it/s, epoch=80, train_loss=0.279]\n",
      "100%|██████████| 468/468 [00:09<00:00, 47.80it/s, epoch=81, train_loss=0.237] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.75it/s, epoch=82, train_loss=0.188] \n",
      "100%|██████████| 468/468 [00:09<00:00, 47.54it/s, epoch=83, train_loss=0.24]  \n",
      "100%|██████████| 468/468 [00:10<00:00, 44.72it/s, epoch=84, train_loss=0.166]\n",
      "100%|██████████| 468/468 [00:11<00:00, 41.92it/s, epoch=85, train_loss=0.185] \n",
      "100%|██████████| 468/468 [00:13<00:00, 34.19it/s, epoch=86, train_loss=0.199] \n",
      "100%|██████████| 468/468 [00:14<00:00, 31.59it/s, epoch=87, train_loss=0.258] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.60it/s, epoch=88, train_loss=0.237] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.79it/s, epoch=89, train_loss=0.223] \n",
      "100%|██████████| 468/468 [00:09<00:00, 46.82it/s, epoch=90, train_loss=0.212] \n",
      "100%|██████████| 468/468 [00:10<00:00, 46.60it/s, epoch=91, train_loss=0.194] \n",
      "100%|██████████| 468/468 [00:11<00:00, 40.30it/s, epoch=92, train_loss=0.28]  \n",
      "100%|██████████| 468/468 [00:12<00:00, 36.78it/s, epoch=93, train_loss=0.266] \n",
      "100%|██████████| 468/468 [00:12<00:00, 38.21it/s, epoch=94, train_loss=0.265]\n",
      "100%|██████████| 468/468 [00:12<00:00, 38.10it/s, epoch=95, train_loss=0.128] \n",
      "100%|██████████| 468/468 [00:12<00:00, 38.55it/s, epoch=96, train_loss=0.185]\n",
      "100%|██████████| 468/468 [00:10<00:00, 43.79it/s, epoch=97, train_loss=0.19]  \n",
      "100%|██████████| 468/468 [00:11<00:00, 41.75it/s, epoch=98, train_loss=0.264]\n",
      "100%|██████████| 468/468 [00:10<00:00, 44.67it/s, epoch=99, train_loss=0.201] \n"
     ]
    }
   ],
   "source": [
    "mlp.train()\n",
    "\n",
    "for e in range(epoches):\n",
    "    t = tqdm(train_loader)\n",
    "    for img, label in t:\n",
    "        # Forward img and compute loss\n",
    "        pred = mlp(img)\n",
    "        loss = criterion(pred, label)\n",
    "        \n",
    "        # TODO: 基于优化器的使用方法，完成反向梯度传播、参数更新\n",
    "        optimizer.zero_grad()#清空梯度\n",
    "        loss.backward()#反向传播\n",
    "        optimizer.step()#更新参数\n",
    "        t.set_postfix(epoch=e, train_loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060fa17",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "\n",
    "- torch.argmax(*input*, *dim*, *keepdim=False*) [Link](https://pytorch.org/docs/stable/generated/torch.argmax.html)\n",
    "  - input: 计算基于的tensor\n",
    "  - dim: 希望按哪个维度求max下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bb449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 50.47it/s, test_acc=0.923]\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "\n",
    "correct_cnt, sample_cnt = 0, 0\n",
    "\n",
    "t = tqdm(test_loader)\n",
    "for img, label in t:\n",
    "    # Predict label for img\n",
    "    img = img.reshape(img.shape[0], -1)\n",
    "    pred = mlp(img)\n",
    "    pred_label = pred.argmax(dim=1)\n",
    "    \n",
    "    correct_cnt += (pred_label == label).sum().item()\n",
    "    sample_cnt += pred_label.shape[0]\n",
    "\n",
    "    t.set_postfix(test_acc=correct_cnt/sample_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251bbfb",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "\n",
    "- 将完成训练的模型保存到服务器的model/目录下\n",
    "\n",
    "- ModelScope服务器端无法长久保存文件，因此请及时下载、本地保存你完成的代码，以及模型的参数文件（model/mlp.pt）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113ac32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model/'):\n",
    "    os.mkdir('model/')\n",
    "\n",
    "torch.save(mlp.state_dict(), 'model/mlp.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0af82aca95c4c45fbffe75f913fb6ef834a764cc29274537834de14d8772671d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
